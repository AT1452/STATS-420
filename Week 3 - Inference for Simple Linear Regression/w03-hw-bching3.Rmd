---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2018, Brandon Ching - bching3"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r}
library("MASS");
library("knitr");
library("kableExtra");
library("dplyr");

# Helper functions
sim_slr = function(x, beta_0 = 10, beta_1 = 5, sigma = 1) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
```

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
cat_model = lm(Hwt ~ Bwt, data = cats);
summary(cat_model)
```

- **Null Hypothesis: There is no significant relationship Bwt and Hwt.**
- **Alternate Hypothesis: There is a significant relationship between Bwt and Hwt.**
- **p-value: < 2.2e-16 **

**(b)** Calculate a 90% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, level = 0.90)
```

- **We are 90% confident that for ever 1 kg in body weight increase, the average increase in heart weight is between 3.6197kg and 4.4484kg.**

**(c)** Calculate a 99% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, level = 0.99)
```

- **we are 99% confident that the average heart weight of a cat weighing 0kg is between -2.164kg and 1.451kg.**

**(d)** Use a 99% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

```{r}
new_cats = data.frame(Bwt = c(2.1, 2.8))
predictions = predict(cat_model, newdata = new_cats, 
  interval = c("confidence"), level = 0.99)

cbind(predictions, length = c(predictions[1, "upr"] - predictions[1, "lwr"], predictions[2, "upr"] - predictions[2, "lwr"]))
```

- **A body weight of 2.1 is wider with a length of 1.03 becuase the body weight of 2.8 is further away from the mean (point of ($\bar{x}$, $\bar{y}$)). **

**(e)** Use a 99% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

```{r}
new_cats = data.frame(Bwt = c(2.8, 4.2))
predictions = predict(cat_model, newdata = new_cats, 
  interval = c("prediction"), level = 0.99)

cbind(predictions, length = c(predictions[1, "upr"] - predictions[1, "lwr"], predictions[2, "upr"] - predictions[2, "lwr"]))
```

- **2.8 is between 7.133 and 14.744. 4.2 is between 12.661 and 20.512**

**(f)** Create a scatterplot of the data. Add the regression line, 90% confidence bands, and 90% prediction bands.

```{r}
cat_grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)
dist_ci_band = predict(cat_model, 
                       newdata = data.frame(Bwt = cat_grid), 
                       interval = "confidence", level = 0.90)
dist_pi_band = predict(cat_model, 
                       newdata = data.frame(Bwt = cat_grid), 
                       interval = "prediction", level = 0.90) 

plot(Hwt ~ Bwt, data = cats,
     xlab = "Body Weight (in kg)",
     ylab = "Heart Weight (in kg)",
     main = "Cat Body Weight vs heart Weight",
     pch  = 20,
     cex  = 2,
     col  = "grey",
     ylim = c(min(dist_pi_band), max(dist_pi_band))
     );
abline(cat_model, lwd = 5, col = "darkorange");

lines(cat_grid, dist_ci_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(cat_grid, dist_ci_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(cat_grid, dist_pi_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 3)
lines(cat_grid, dist_pi_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 3)
points(mean(cats$Bwt), mean(cats$Hwt), pch = "+", cex = 3)
```

**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
coefficients = summary(cat_model)$coefficients
customt = (coefficients[2, "Estimate"] - 4)/coefficients[2, "Std. Error"];
pval = 2 * pt(abs(customt), df = length(resid(cat_model)) - 2, lower.tail = FALSE)
```

- **Test Statistic: `r customt`**
- **p-value: `r pval`**
- **Becase the p-value is greater than an $\alpha = 0.05$, we must fail to reject $H_0.**

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
library("mlbench");
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]


Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
ozone_wind_model = lm(ozone ~ wind, data = Ozone);
summary(ozone_wind_model);
```

- **Null Hypothesis: There is no significant relationship wind and ozone levels.**
- **Alternate Hypothesis: There is a significant relationship between wind and ozone levels.**
- **t: -0.219**
- **p-value: 0.827**
- **With $\alpha = 0.01$ we fail to reject $H_0 as the p-value is larger than $\alpha$**
- **Conclusion is that wind does not appear to have any relation to ozone levels.**

**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
ozone_temp_model = lm(ozone ~ temp, data = Ozone);
summary(ozone_temp_model);
```

- **Null Hypothesis: There is no significant relationship temp and ozone levels.**
- **Alternate Hypothesis: There is a significant relationship between temp and ozone levels.**
- **t: 22.85**
- **p-value: < 2e-16**
- **With $\alpha = 0.01$ we can reject $H_0 as the p-value is much smaller than $\alpha$**
- **Conclusion is that temp does appear to have a relation to ozone levels.**

***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19810908;
set.seed(birthday);
n = 50;
x = seq(0, 10, length = n);
beta_0 = -5;
beta_1 = 3.25;
sigma = 4;
beta_hat_0 = rep(0, 2000);
beta_hat_1 = rep(0, 2000);
sxx = sum((x - mean(x))^2);

for(i in 1:2000){
  sim_data = sim_slr(x, beta_0 = beta_0, beta_1 = beta_1, sigma = sigma);
  sim_model = lm(response ~ predictor, data = sim_data);
  
  beta_hat_0[i] = coef(sim_model)[1];
  beta_hat_1[i] = coef(sim_model)[2];
}
```

**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

```{r}
sdb0 = sigma * sqrt((1/length(x) + (mean(x)^2/sxx)));
sdb1 = sigma/sqrt(sxx);

col1 = c("Expected" = beta_0, "Mean" = mean(beta_hat_0), "Known StdDv." = sdb0, "Sim StdDv" = sd(beta_hat_0));
col2 = c("Expected" = beta_1, "Mean" = mean(beta_hat_1), "Known StdDv." = sdb1, "Sim StdDv" = sd(beta_hat_1));
table = data.frame(beta_hat_0 = col1, beta_hat_1 = col2);

kable(table, row.names = TRUE);
```

**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.

```{r}
hist(beta_hat_0, 
     prob = TRUE, 
     breaks = 25, 
     xlab = expression(hat(beta)[0]),
     main = "",
     border = "dodgerblue"
     );
curve(dnorm(x, mean = beta_0, sd = sdb0), 
      col = "darkorange",
      add = TRUE,
      lwd = 3
      );

hist(beta_hat_1, 
     prob = TRUE, 
     breaks = 25, 
     xlab = expression(hat(beta)[1]),
     main = "",
     border = "dodgerblue"
     );
curve(dnorm(x, mean = beta_1, sd = sdb1), 
      col = "darkorange",
      add = TRUE,
      lwd = 3
      );
```

***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19810908
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)

beta_0 = 5;
beta_1 = 2;
sigma = 3;
beta_hat_1 = rep(0, 2500);
s_e = rep(0, 2500);
sxx = sum((x - mean(x))^2);
var_beta_1_hat = sigma ^ 2 / sxx

for(i in 1:2500){
  sim_data = sim_slr(x, beta_0 = beta_0, beta_1 = beta_1, sigma = sigma);
  sim_model = lm(response ~ predictor, data = sim_data);
  
  beta_hat_1[i] = coef(sim_model)[2];
  s_e[i] = summary(sim_model)$coefficients[2, 2];
}
```

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

```{r}
confint(sim_model, level = 0.95)
cv = qt(0.95, df = length(beta_hat_1) - 2);
lower_95 = beta_hat_1 - cv * s_e;
upper_95 = beta_hat_1 + cv * s_e;

intervals = data.frame("lower" = lower_95, "upper" = upper_95);
head(intervals)
```

**(c)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
within_range = filter(intervals, lower <= beta_1, upper >= beta_1);

(nrow(within_range)/nrow(intervals))
```

- **88.6% contain the true value of $\beta_1$**

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

- **11.4% of the intervals would reject $H_0$**

**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

```{r}
confint(sim_model, level = 0.99)
cv = qt(0.99, df = length(beta_hat_1) - 2);
lower_99 = beta_hat_1 - cv * s_e;
upper_99 = beta_hat_1 + cv * s_e;

intervals = data.frame("lower" = lower_99, "upper" = upper_99);
head(intervals)
```

**(f)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
within_range = filter(intervals, lower <= beta_1, upper >= beta_1);

(nrow(within_range)/nrow(intervals))
```

- **97.3% contain the true value of $\beta_1$**

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

- **2.7% of the intervals would reject $H_0$**

***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

```{r}
calc_pred_int = function (model, newdata, level = 0.95) {
  n = nrow(model$model);
  x = as.vector(model$model[,2]);
  y = as.vector(model$model[,1]);
  x_bar = mean(x);
  
  beta_hat_1 = coef(model)[[2]];
  beta_hat_0 = coef(model)[[1]];
  y_hat = beta_hat_0 + beta_hat_1 * x;
  e = y - y_hat;
  
  estimate = beta_hat_0 + beta_hat_1 * newdata[[1]];
  se = sqrt(sum(e^2) / (n-2))
  Sxx = sum((x - x_bar)^2);
  
  pred = se * sqrt(1 + ( 1/length(x) )  + ( (newdata[[1]] - x_bar)^2 / Sxx ) );
  c("estimate" = estimate, "lower" = estimate - pred, "upper" = estimate + pred);
}

```

- **I know this is incorrect but I have no idea where the issue is.**

**(b)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```

**(c)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.99)
```


